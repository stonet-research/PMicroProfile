# General Notes
- Need kernel version >= 4.15 for MAP_SYNC support when pmen=on, ensures file metadata stays in sync in case of a crash.
- Chameleon has a nice set up for measuring micro-architecture events (https://arxiv.org/abs/2206.02878). Also, it suggests setting the sampling rate of the PMU to one sample for each 200 events.

# Notes Wednesday Morning Meeting

- Keep slides simple, use visual elements to mark important events/elements.
- Add slide explaning relevant to computer manifesto ;)
- Icon set streamlined;
- Pitch in two weeks time;

# Animesh meeting (22 februari)

- Proposal check;
- Scope of benchmarking tool (broad; support every FS (driver level), specific: only support ctFS/UFS, maybe hybrid?);
- Perma-Bench;
- Need real hardware;


## Notes

Varmail bench; on wait level you can track micro arch events.
Replace the workload of varmail in perma-bench.

Unknown: what to capture?


Questions:
- Memory access patterns: log read/write at the device level. What is the level of tracing/granularity we can detect writes?
- Rerun the trace in user space, find interesting events.
- Read paper "unwritten contract of storage ssd & flash ssd's" (https://pages.cs.wisc.edu/~jhe/eurosys17-he.pdf, https://www.usenix.org/conference/hotstorage19/presentation/wu-kan).

## Context
Older benchmarks may not be able to capture the micro-architectual events occuring within file systems like ctFS?

## First part of the work (sub-question 1?)

First, we should be able to extract the memory access patterns, not the software read/write/other kind of events that are we are dealing with in other file systems. 
We could log the read/write in a virtual environment (QEMU) by testing whether a region in the backing file is written. A potential workload would be varmail, in combination with SplitFS. Performing this kind of logging with a baremetal systems might be more difficult, so we have to research whether this is possible. Maybe we can check if the page table bit is dirty in MMAP'ed backed 

Idea: visualize the access patterns using a graph.

--> Sub-question: How should one capture micro-arch events for PMEM? (just an idea) 

## Second part of the work (sub-question 2?)

Second, we use this trace to replay the workload using a benchmarking framework like Perma-Bench. We use this trace, in combination with appropiate event counters (have to research!), to idenfify hot spots.


EXAMPLE:
--------

Trace:  		RRWWWRRRRRWWWWWWWWWRWRWWRWRWRWWW
Events; Cache misses:   ---------#---#--##--X-----------
	Page fault:	--X----X----X--------XXX--------
	ctFS events:	---I----------------------------
		(I: Insert PMD?)


--> Sub-question:

## Third part of the work (sub-question 3?)

Finally, we use these insights to optimize 'ctFS' or another file system, e.g. 'UFS'.


# Notes weekly storage meeting: 24 february
Questions:
- We generate a workload by using a 'stable' file system (e.g. SplitFS) to generate a trace, which then can then used to stress test ctFS. However, are we going to use this workload to find interesting bottlenecks, however, now it also contains SplitFS's metadata junk? Isn't this a problem? Second question: this workload will run on top of ctFS, so we probably should allocate on giant backing file.

My questions:

- Are we are going to capture the real data written to disk? Or only how large the read/writes are.




Sub-question: how one should capture micro-arch events for PMEM

